{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b91ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tools import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ed0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_status_estacion_mes(config:dict):\n",
    "    if os.path.exists(f'{config.path}/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv'):\n",
    "\n",
    "        data_df = pd.read_csv(f'{config.path}/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv')\n",
    "\n",
    "        intial_size = data_df.shape[0]\n",
    "        print(data_df.shape)\n",
    "\n",
    "        # change column to one hot enconding\n",
    "        data_df['is_charging_station'] = data_df.is_charging_station.astype(np.int)\n",
    "\n",
    "        # STATUS = IN_SERVICE=En servei, CLOSED=Tancada, MAINTENANCE=installed but closed for MAINTENANCE, PLANNED=not installed and closed\n",
    "        # replace IN_SERVICE with 1 and CLOSED with 0 \n",
    "        data_df['status'].replace(\n",
    "            to_replace=['IN_SERVICE', 'OPEN', 'OPN', 'CLS', 'CLOSED', 'NOT_IN_SERVICE', 'MAINTENANCE', 'PLANNED'],                       \n",
    "            value=[0, 0, 0, 1, 1, 1,  2, 3], inplace=True)\n",
    "\n",
    "        data_df.loc[data_df.last_reported.isna(), 'last_reported'] = data_df.loc[data_df.last_reported.isna(), 'last_updated']\n",
    "\n",
    "        # will remove the duplicate for last reported for all stations in the dataset\n",
    "        data_df = remove_duplicates_all(data_df.copy(), 'last_reported')\n",
    "\n",
    "        # convert timestamps of last_updated\n",
    "        data_df = convert_timestamp(data_df.copy(), ['last_updated'], sort=True, add=True)\n",
    "\n",
    "        # convert timestamps to multimple of 60\n",
    "        data_df = timestamp_multipleof(\n",
    "            devide_by=config.devide_by, \n",
    "            column='minutes_last_updated_date',\n",
    "            df=data_df.copy(), \n",
    "            new_column='last_updated', \n",
    "            year_column='year_last_updated_date',\n",
    "            month_column='month_last_updated_date',\n",
    "            day_column='dayofmonth_last_updated_date',\n",
    "            hour_column='hour_last_updated_date',\n",
    "            minutes_column='minutes_last_updated_date'\n",
    "        )\n",
    "\n",
    "        # print(data_df.minutes_last_updated_date.value_counts())\n",
    "        data_df.drop(['minutes_last_updated_date', 'week_last_updated_date'], axis=1, inplace=True)\n",
    "\n",
    "        ### will remove the duplicate for last reported for all stations in the dataset\n",
    "        data_df = remove_duplicates_all(data_df.copy(), 'last_updated')\n",
    "\n",
    "        print(data_df.shape)\n",
    "        print('removed:', intial_size-data_df.shape[0])\n",
    "\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        data_df.drop(['ttl'], axis=1, inplace=True)\n",
    "\n",
    "        # save checkpoint\n",
    "        data_df.to_csv(f'{config.path}/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv', index=False)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "def read_status_informacio_mes(config:dict) -> dd.core.DataFrame:\n",
    "    if os.path.exists(f'{config.path}/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv'):\n",
    "        \n",
    "        data_df = pd.read_csv(f'{config.path}/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv')\n",
    "\n",
    "        intial_size = data_df.shape[0]\n",
    "        print(data_df.shape)\n",
    "\n",
    "        # drop not needed columns\n",
    "        # data_df.drop(['nearbyStations', 'cross_street'], axis=1, inplace=True)\n",
    "\n",
    "        data_df.loc[data_df.altitude.isin(['0.1', 'nan', np.nan]), 'altitude'] = '0'\n",
    "        data_df.altitude = data_df.altitude.astype(np.int).astype(str)\n",
    "\n",
    "        cond = (~data_df.altitude.isin([str(x) for x in range(200)] + [np.nan]))\n",
    "        print(data_df[cond].shape)\n",
    "        # 485 row does not have 0 in the altitud column\n",
    "        # capacity is filled with values 1 to fix this we need to shift the data \n",
    "\n",
    "        # Fix data \n",
    "        data_df.loc[cond, ['capacity']] = data_df[cond].post_code\n",
    "        data_df.loc[cond, ['post_code']] = data_df[cond].address\n",
    "        data_df.loc[cond, ['address']] = data_df[cond].altitude\n",
    "        data_df.loc[cond, ['altitude']] = '0'\n",
    "        data_df.altitude.fillna('0', inplace=True)\n",
    "\n",
    "        # post code is wrong need fixing using long & lat. \n",
    "        # can be fixed using post code data from old dataset after the merge\n",
    "        data_df['post_code'] = '0'\n",
    "\n",
    "        data_df = convert_timestamp(data_df.copy(), ['last_updated'], sort=True, add=True)\n",
    "\n",
    "        # convert timestamps to multimple of 3\n",
    "        data_df = timestamp_multipleof(\n",
    "            devide_by=config.devide_by, \n",
    "            column='minutes_last_updated_date',\n",
    "            df=data_df.copy(), \n",
    "            new_column='last_updated', \n",
    "            year_column='year_last_updated_date',\n",
    "            month_column='month_last_updated_date',\n",
    "            day_column='dayofmonth_last_updated_date',\n",
    "            hour_column='hour_last_updated_date',\n",
    "            minutes_column='minutes_last_updated_date'\n",
    "        )\n",
    "\n",
    "        # drop not needed columns\n",
    "        data_df.drop(\n",
    "            [\n",
    "                'year_last_updated_date', 'month_last_updated_date',\n",
    "                'week_last_updated_date', 'dayofweek_last_updated_date',\n",
    "                'dayofmonth_last_updated_date', 'dayofyear_last_updated_date',\n",
    "                'hour_last_updated_date', 'minutes_last_updated_date'\n",
    "            ],\n",
    "            axis=1,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        data_df['physical_configuration'].replace(to_replace=['REGULAR', 'BIKE','BIKESTATION', 'BIKE-ELECTRIC', 'ELECTRICBIKESTATION'], value=[0, 0, 0, 1, 1], inplace=True)\n",
    "\n",
    "        # create mew column of last reported and last updated \n",
    "        data_df['street_name'] = data_df.apply(\n",
    "            lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x['name'])),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        def lambda_fun(name):\n",
    "            ret = 'nan'\n",
    "            try:\n",
    "                ret = re.findall(\"\\d+$\", name)[0]\n",
    "            except:\n",
    "                ret = 'nan'\n",
    "\n",
    "            return ret\n",
    "\n",
    "        # create mew column of last reported and last updated \n",
    "        data_df['street_number'] = data_df.apply(\n",
    "            lambda x: lambda_fun(x['name']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # we don't have this column anywhere in the new dataset so it got removed\n",
    "        data_df.drop(['address', 'name'], axis=1, inplace=True)\n",
    "\n",
    "        ### will remove the duplicate for last reported for all stations in the dataset\n",
    "        data_df = remove_duplicates_all(data_df.copy(), 'last_updated')\n",
    "\n",
    "        print(data_df.shape)\n",
    "        print('removed:', intial_size-data_df.shape[0])\n",
    "\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        data_df.drop(['ttl'], axis=1, inplace=True)\n",
    "\n",
    "        # save checkpoint\n",
    "        data_df.to_csv(f'{config.path}/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv', index=False)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "def get_file_length(config:dict):\n",
    "    data_df = pd.read_csv(\n",
    "        filepath_or_buffer=f'../dades/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv',\n",
    "        header=0,\n",
    "        low_memory=False,\n",
    "    )\n",
    "    return data_df.shape\n",
    "    \n",
    "def read_informacion_estacion_anual(input_dataset:str, year:int, path:str='dades'):\n",
    "    assert input_dataset != \"\"\n",
    "    assert year >= 2018 and year <= 2023\n",
    "\n",
    "    config = pd.Series({\n",
    "        'devide_by':60,\n",
    "        'year':year,\n",
    "        'datafrom': input_dataset,\n",
    "        'dataset': f'{input_dataset}_CLEAN',\n",
    "        'ttl': 30,\n",
    "        'month': np.nan,\n",
    "        'monthname': np.nan,\n",
    "        'path':path\n",
    "    })\n",
    "\n",
    "    os.system(f\"mkdir -p {config.path}/{config.year}/{config.dataset}\")\n",
    "\n",
    "    for month, month_name in i2m:\n",
    "        config.month = month\n",
    "        config.monthname = month_name\n",
    "        print(year, month, month_name, input_dataset)\n",
    "        if not os.path.exists(f'{config.path}/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv'):\n",
    "            if input_dataset == 'BicingNou_ESTACIONS':\n",
    "                read_status_estacion_mes(config)\n",
    "            elif input_dataset == 'BicingNou_INFORMACIO':\n",
    "                read_status_informacio_mes(config)\n",
    "            # TODO add elif para cada dataset que queramso anadir en el futuro ()\n",
    "        else:\n",
    "            print('found file with shape equal to: ', get_file_length(config))\n",
    "            \n",
    "        print('Done -------- ----------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e5d8b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 Gener BicingNou_ESTACIONS\n",
      "File not found\n",
      "Done -------- ----------\n",
      "2019 2 Febrer BicingNou_ESTACIONS\n",
      "File not found\n",
      "Done -------- ----------\n",
      "2019 3 Marc BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (355467, 13)\n",
      "Done -------- ----------\n",
      "2019 4 Abril BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3018524, 13)\n",
      "Done -------- ----------\n",
      "2019 5 Maig BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3553843, 13)\n",
      "Done -------- ----------\n",
      "2019 6 Juny BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3466316, 13)\n",
      "Done -------- ----------\n",
      "2019 7 Juliol BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3238510, 13)\n",
      "Done -------- ----------\n",
      "2019 8 Agost BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3660322, 13)\n",
      "Done -------- ----------\n",
      "2019 9 Setembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3518215, 13)\n",
      "Done -------- ----------\n",
      "2019 10 Octubre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3655823, 13)\n",
      "Done -------- ----------\n",
      "2019 11 Novembre BicingNou_ESTACIONS\n",
      "(3516980, 13)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c190af6acc41e6a52e0fd6d899208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122fccf5de03423187275f829e43d100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291368, 19)\n",
      "removed: 3225612\n",
      "Done -------- ----------\n",
      "2019 12 Desembre BicingNou_ESTACIONS\n",
      "(3655150, 13)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2bfb8d67034327b907b05899141e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a0cfea84874a349a60a37b08e139be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301527, 19)\n",
      "removed: 3353623\n",
      "Done -------- ----------\n",
      "CPU times: user 40min 17s, sys: 5.65 s, total: 40min 23s\n",
      "Wall time: 40min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2019, '../dades')\n",
    "\n",
    "# TODO work todo, clean data and prepare all month"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27859f99",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_INFORMACIO', 2019, '../dades')\n",
    "\n",
    "# TODO work todo, clean data and prepare all month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3e6efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 1 Gener BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3398708, 13)\n",
      "Done -------- ----------\n",
      "2020 2 Febrer BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3500841, 13)\n",
      "Done -------- ----------\n",
      "2020 3 Marc BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (2408419, 13)\n",
      "Done -------- ----------\n",
      "2020 4 Abril BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3892389, 13)\n",
      "Done -------- ----------\n",
      "2020 5 Maig BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3742388, 13)\n",
      "Done -------- ----------\n",
      "2020 6 Juny BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4099864, 13)\n",
      "Done -------- ----------\n",
      "2020 7 Juliol BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4335089, 13)\n",
      "Done -------- ----------\n",
      "2020 8 Agost BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3869580, 13)\n",
      "Done -------- ----------\n",
      "2020 9 Setembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4246500, 13)\n",
      "Done -------- ----------\n",
      "2020 10 Octubre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4455674, 13)\n",
      "Done -------- ----------\n",
      "2020 11 Novembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3670067, 13)\n",
      "Done -------- ----------\n",
      "2020 12 Desembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4419114, 13)\n",
      "Done -------- ----------\n",
      "CPU times: user 17.2 s, sys: 2.52 s, total: 19.7 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2020, '../dades')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1502a7a0",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_INFORMACIO', 2020, '../dades')\n",
    "\n",
    "# TODO work todo, clean data and prepare all month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069c66e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 1 Gener BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4509149, 13)\n",
      "Done -------- ----------\n",
      "2021 2 Febrer BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4017900, 13)\n",
      "Done -------- ----------\n",
      "2021 3 Marc BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4528030, 13)\n",
      "Done -------- ----------\n",
      "2021 4 Abril BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4389888, 13)\n",
      "Done -------- ----------\n",
      "2021 5 Maig BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4538684, 13)\n",
      "Done -------- ----------\n",
      "2021 6 Juny BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4308619, 13)\n",
      "Done -------- ----------\n",
      "2021 7 Juliol BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4436678, 13)\n",
      "Done -------- ----------\n",
      "2021 8 Agost BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4520650, 13)\n",
      "Done -------- ----------\n",
      "2021 9 Setembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4384751, 13)\n",
      "Done -------- ----------\n",
      "2021 10 Octubre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4501053, 13)\n",
      "Done -------- ----------\n",
      "2021 11 Novembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4315064, 14)\n",
      "Done -------- ----------\n",
      "2021 12 Desembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4398576, 14)\n",
      "Done -------- ----------\n",
      "CPU times: user 20.8 s, sys: 2.84 s, total: 23.6 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2021, '../dades')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3652dcb",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_INFORMACIO', 2021, '../dades')\n",
    "\n",
    "# TODO work todo, clean data and prepare all month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fccde87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 1 Gener BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4390690, 14)\n",
      "Done -------- ----------\n",
      "2022 2 Febrer BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (3936804, 14)\n",
      "Done -------- ----------\n",
      "2022 3 Marc BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4535987, 14)\n",
      "Done -------- ----------\n",
      "2022 4 Abril BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4371369, 14)\n",
      "Done -------- ----------\n",
      "2022 5 Maig BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4523985, 14)\n",
      "Done -------- ----------\n",
      "2022 6 Juny BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4382454, 14)\n",
      "Done -------- ----------\n",
      "2022 7 Juliol BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4524353, 14)\n",
      "Done -------- ----------\n",
      "2022 8 Agost BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4497691, 14)\n",
      "Done -------- ----------\n",
      "2022 9 Setembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4362921, 14)\n",
      "Done -------- ----------\n",
      "2022 10 Octubre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4508304, 14)\n",
      "Done -------- ----------\n",
      "2022 11 Novembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4345375, 14)\n",
      "Done -------- ----------\n",
      "2022 12 Desembre BicingNou_ESTACIONS\n",
      "found file with shape equal to:  (4512523, 14)\n",
      "Done -------- ----------\n",
      "CPU times: user 21.6 s, sys: 1.91 s, total: 23.5 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2022, '../dades')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "502a0aab",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_INFORMACIO', 2022, '../dades')\n",
    "\n",
    "# TODO work todo, clean data and prepare all month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b9ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b91ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tools.tools import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed1797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "month_names = ['Gener','Febrer','Marc','Abril','Maig','Juny','Juliol','Agost','Setembre','Octubre','Novembre','Desembre']\n",
    "months = range(1,13)\n",
    "i2m = list(zip(months, month_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ae6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_status_estacion_mes(config:dict):\n",
    "    data_df = pd.read_csv(f'../dades/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv')\n",
    "\n",
    "    intial_size = data_df.shape[0]\n",
    "    print(data_df.shape)\n",
    "\n",
    "    # change column to one hot enconding\n",
    "    data_df['is_charging_station'] = data_df.is_charging_station.astype(np.int)\n",
    "\n",
    "    # STATUS = IN_SERVICE=En servei, CLOSED=Tancada, MAINTENANCE=installed but closed for MAINTENANCE, PLANNED=not installed and closed\n",
    "    # replace IN_SERVICE with 1 and CLOSED with 0 \n",
    "    data_df['status'].replace(\n",
    "        to_replace=['IN_SERVICE', 'OPEN', 'OPN', 'CLS', 'CLOSED', 'NOT_IN_SERVICE', 'MAINTENANCE', 'PLANNED'],                       \n",
    "        value=[0, 0, 0, 1, 1, 1,  2, 3], inplace=True)\n",
    "    \n",
    "    data_df.loc[data_df.last_reported.isna(), 'last_reported'] = data_df.loc[data_df.last_reported.isna(), 'last_updated']\n",
    "\n",
    "    # will remove the duplicate for last reported for all stations in the dataset\n",
    "    data_df = remove_duplicates_all(data_df.copy(), 'last_reported')\n",
    "\n",
    "    # convert timestamps of last_updated\n",
    "    data_df = convert_timestamp(data_df.copy(), ['last_updated'], sort=True, add=True)\n",
    "\n",
    "    # convert timestamps to multimple of 60\n",
    "    data_df = timestamp_multipleof(\n",
    "        devide_by=config.devide_by, \n",
    "        column='minutes_last_updated_date',\n",
    "        df=data_df.copy(), \n",
    "        new_column='last_updated', \n",
    "        year_column='year_last_updated_date',\n",
    "        month_column='month_last_updated_date',\n",
    "        day_column='dayofmonth_last_updated_date',\n",
    "        hour_column='hour_last_updated_date',\n",
    "        minutes_column='minutes_last_updated_date'\n",
    "    )\n",
    "    \n",
    "    # print(data_df.minutes_last_updated_date.value_counts())\n",
    "    data_df.drop(['minutes_last_updated_date'], axis=1, inplace=True)\n",
    "\n",
    "    ### will remove the duplicate for last reported for all stations in the dataset\n",
    "    data_df = remove_duplicates_all(data_df.copy(), 'last_updated')\n",
    "    \n",
    "    print(data_df.shape)\n",
    "    print('removed:', intial_size-data_df.shape[0])\n",
    "    \n",
    "    data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_df.drop(['ttl'], axis=1, inplace=True)\n",
    "\n",
    "    # save checkpoint\n",
    "    data_df.to_csv(f'../dades/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b2c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_status_informacio_mes(config:dict) -> dd.core.DataFrame:\n",
    "\n",
    "    data_df = pd.read_csv(f'../dades/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv')\n",
    "\n",
    "    intial_size = data_df.shape[0]\n",
    "    print(data_df.shape)\n",
    "    \n",
    "    # drop not needed columns\n",
    "    # data_df.drop(['nearbyStations', 'cross_street'], axis=1, inplace=True)\n",
    "\n",
    "    data_df.loc[data_df.altitude.isin(['0.1', 'nan', np.nan]), 'altitude'] = '0'\n",
    "    data_df.altitude = data_df.altitude.astype(np.int).astype(str)\n",
    "\n",
    "    cond = (~data_df.altitude.isin([str(x) for x in range(200)] + [np.nan]))\n",
    "    print(data_df[cond].shape)\n",
    "    # 485 row does not have 0 in the altitud column\n",
    "    # capacity is filled with values 1 to fix this we need to shift the data \n",
    "\n",
    "    # Fix data \n",
    "    data_df.loc[cond, ['capacity']] = data_df[cond].post_code\n",
    "    data_df.loc[cond, ['post_code']] = data_df[cond].address\n",
    "    data_df.loc[cond, ['address']] = data_df[cond].altitude\n",
    "    data_df.loc[cond, ['altitude']] = '0'\n",
    "    data_df.altitude.fillna('0', inplace=True)\n",
    "\n",
    "    # post code is wrong need fixing using long & lat. \n",
    "    # can be fixed using post code data from old dataset after the merge\n",
    "    data_df['post_code'] = '0'\n",
    "\n",
    "    data_df = convert_timestamp(data_df.copy(), ['last_updated'], sort=True, add=True)\n",
    "\n",
    "    # convert timestamps to multimple of 3\n",
    "    data_df = timestamp_multipleof(\n",
    "        devide_by=config.devide_by, \n",
    "        column='minutes_last_updated_date',\n",
    "        df=data_df.copy(), \n",
    "        new_column='last_updated', \n",
    "        year_column='year_last_updated_date',\n",
    "        month_column='month_last_updated_date',\n",
    "        day_column='dayofmonth_last_updated_date',\n",
    "        hour_column='hour_last_updated_date',\n",
    "        minutes_column='minutes_last_updated_date'\n",
    "    )\n",
    "\n",
    "    # drop not needed columns\n",
    "    data_df.drop(\n",
    "        [\n",
    "            'year_last_updated_date', 'month_last_updated_date',\n",
    "            'week_last_updated_date', 'dayofweek_last_updated_date',\n",
    "            'dayofmonth_last_updated_date', 'dayofyear_last_updated_date',\n",
    "            'hour_last_updated_date', 'minutes_last_updated_date'\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    data_df['physical_configuration'].replace(to_replace=['REGULAR', 'BIKE','BIKESTATION', 'BIKE-ELECTRIC', 'ELECTRICBIKESTATION'], value=[0, 0, 0, 1, 1], inplace=True)\n",
    "\n",
    "    # create mew column of last reported and last updated \n",
    "    data_df['street_name'] = data_df.apply(\n",
    "        lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x['name'])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    def lambda_fun(name):\n",
    "        ret = 'nan'\n",
    "        try:\n",
    "            ret = re.findall(\"\\d+$\", name)[0]\n",
    "        except:\n",
    "            ret = 'nan'\n",
    "\n",
    "        return ret\n",
    "\n",
    "    # create mew column of last reported and last updated \n",
    "    data_df['street_number'] = data_df.apply(\n",
    "        lambda x: lambda_fun(x['name']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # we don't have this column anywhere in the new dataset so it got removed\n",
    "    data_df.drop(['address', 'name'], axis=1, inplace=True)\n",
    "\n",
    "    ### will remove the duplicate for last reported for all stations in the dataset\n",
    "    data_df = remove_duplicates_all(data_df.copy(), 'last_updated')\n",
    "    \n",
    "    print(data_df.shape)\n",
    "    print('removed:', intial_size-data_df.shape[0])\n",
    "    \n",
    "    data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_df.drop(['ttl'], axis=1, inplace=True)\n",
    "\n",
    "    # save checkpoint\n",
    "    data_df.to_csv(f'../dades/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_length(config:dict):\n",
    "    data_df = pd.read_csv(\n",
    "        filepath_or_buffer=f'../dades/{config.year}/{config.datafrom}/{config.year}_{config.month:02d}_{config.monthname}_{config.datafrom}.csv',\n",
    "        header=0,\n",
    "        low_memory=False,\n",
    "    )\n",
    "    return data_df.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_informacion_estacion_anual(input_dataset:str, year:int):\n",
    "    assert input_dataset != \"\"\n",
    "    assert year >= 2018 and year <= 2023\n",
    "\n",
    "    config = pd.Series({\n",
    "        'devide_by':60,\n",
    "        'year':year,\n",
    "        'datafrom': input_dataset,\n",
    "        'dataset': f'{input_dataset}_CLEAN',\n",
    "        'ttl': 30,\n",
    "        'month': np.nan,\n",
    "        'monthname': np.nan\n",
    "    })\n",
    "\n",
    "    os.system(f\"mkdir -p ../dades/{config.year}/{config.dataset}\")\n",
    "\n",
    "    for month, month_name in i2m:\n",
    "        config.month = month\n",
    "        config.monthname = month_name\n",
    "        print(year, month, month_name, input_dataset)\n",
    "        if not os.path.exists(f'../dades/{config.year}/{config.dataset}/{config.year}_{config.month:02d}_{config.monthname}_{config.dataset}.csv'):\n",
    "            if input_dataset == 'BicingNou_ESTACIONS':\n",
    "                read_status_estacion_mes(config)\n",
    "            elif input_dataset == 'BicingNou_INFORMACIO':\n",
    "                read_status_informacio_mes(config)\n",
    "            # TODO add elif para cada dataset que queramso anadir en el futuro ()\n",
    "        else:\n",
    "            print('found file with shape equal to: ', get_file_length(config))\n",
    "            \n",
    "        print('Done -------- ----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069c66e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 1 Gener BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 2 Febrer BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 3 Marc BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 4 Abril BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 5 Maig BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 6 Juny BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 7 Juliol BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 8 Agost BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 9 Setembre BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 10 Octubre BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 11 Novembre BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "2021 12 Desembre BicingNou_ESTACIONS\n",
      "Done -------- ----------\n",
      "CPU times: user 11.2 ms, sys: 20 µs, total: 11.2 ms\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccde87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This function will generate the database cleansed and ready to explore using dask\n",
    "read_informacion_estacion_anual('BicingNou_ESTACIONS', 2022)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
